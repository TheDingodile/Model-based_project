{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0100ac6b",
   "metadata": {},
   "source": [
    "# Model Based Machine Learning Project\n",
    "\n",
    "We have chosen to work with a dataset on Life Expectancy collected by (WHO)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022c4aaa",
   "metadata": {},
   "source": [
    "## Loadning and visualizing the data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19dc7330",
   "metadata": {},
   "source": [
    "We start by importing packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8c8350",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing\n",
    "import statsmodels.api as sm\n",
    "import random\n",
    "\n",
    "#Plotting\n",
    "import matplotlib as mplib\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "palette = itertools.cycle(sns.color_palette(\"tab20\", n_colors=20))\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "#Machine learning \n",
    "from sklearn import linear_model\n",
    "import torch\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "from pyro.contrib.autoguide import AutoDiagonalNormal, AutoMultivariateNormal\n",
    "from pyro.infer import MCMC, NUTS, HMC, SVI, Trace_ELBO\n",
    "from pyro.optim import Adam, ClippedAdam\n",
    "\n",
    "# fix random generator seed (for reproducibility of results)\n",
    "np.random.seed(42)\n",
    "\n",
    "# matplotlib style options\n",
    "plt.style.use('ggplot')\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (6, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b654c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the data\n",
    "data = pd.read_csv(\"archive/Data_processed_subregions.csv\",delimiter=\";\")\n",
    "#Formatting into pandas dataframe \n",
    "LED = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4e2b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(LED.info()) #Check quality of data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c3dcdc",
   "metadata": {},
   "source": [
    "## Setting up data for Pyro model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6420a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "LED.head(5) #Display first 5 row to get idea of the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75046c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = LED.values #Convert dataframe into matrix \n",
    "\n",
    "X = mat[:,6:].astype(\"float\") #These are the regressors\n",
    "print(X.shape)\n",
    "\n",
    "y_original = mat[:,3].astype(\"float\") #This is the target variable life expectancy\n",
    "print(y_original.shape)\n",
    "\n",
    "#These are the hierarchies\n",
    "continent = mat[:,1] \n",
    "subregion = mat[:,2] \n",
    "developed = mat[:,5]\n",
    "print(continent.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0634cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setup split for traning, validation and test data set. We will use 2000-2012 as traning and 2013-2015 as test. \n",
    "#Remember to add in report: We can not split randomly as then we will feed info to the model I should not have. \n",
    "training_idx = (data['Year'] >= 2000) & (data['Year'] <= 2011)\n",
    "test_idx = (data['Year'] >= 2012) & (data['Year'] <= 2013)\n",
    "val_idx = (data['Year'] >= 2014) & (data['Year'] <= 2015)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3daf92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize input features\n",
    "X_mean = X[training_idx,:].mean(axis=0)\n",
    "X_std = X[training_idx,:].std(axis=0)\n",
    "X = (X - X_mean) / X_std\n",
    "\n",
    "# standardize target\n",
    "y_mean = y_original[training_idx].mean()\n",
    "y_std = y_original[training_idx].std()\n",
    "y = (y_original - y_mean) / y_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5dd8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot target variable before and after standardization \n",
    "plt.plot(y_original, label='True Values')\n",
    "plt.plot(y, label='Standardized values')\n",
    "plt.xlabel('Index')\n",
    "plt.ylabel('Value')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13fa64fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataSplit(data,trainIDX,testIDX,valIDX): #Define function to split data\n",
    "    train = data[trainIDX]\n",
    "    test  = data[testIDX]\n",
    "    val   = data[valIDX]\n",
    "    \n",
    "    return train, test, val\n",
    "\n",
    "#Split features \n",
    "X_train = X[training_idx,:]\n",
    "X_test = X[test_idx,:]\n",
    "X_val = X[val_idx, :]\n",
    "\n",
    "#Split observations\n",
    "y_train, y_test, y_val = dataSplit(y,training_idx,test_idx,val_idx)\n",
    "\n",
    "print(\"num train: %d\" % len(y_train))\n",
    "print(\"num test: %d\" % len(y_test))\n",
    "print(\"num val: %d\" % len(y_val))\n",
    "\n",
    "developed_train, developed_test, developed_val = dataSplit(developed,training_idx,test_idx,val_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236f1732",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up hierarchical dictonaries \n",
    "\n",
    "#Contienent dictonaries\n",
    "continent_dict = {'Asia': 0, 'Europe': 1, 'Africa': 2, 'North America': 3,  'South America': 4, 'Oceania': 5}\n",
    "\n",
    "#Sub-region dictonaries\n",
    "subregion_dict = region_dict = {\n",
    "    'Southern Asia': 0,\n",
    "    'Southern Europe': 1,\n",
    "    'Northern Africa': 2,\n",
    "    'Sub-Saharan Africa': 3,\n",
    "    'Latin America and the Caribbean': 4,\n",
    "    'Western Asia': 5,\n",
    "    'Australia and New Zealand': 6,\n",
    "    'Western Europe': 7,\n",
    "    'Eastern Europe': 8,\n",
    "    'South-eastern Asia': 9,\n",
    "    'Northern America': 10,\n",
    "    'Eastern Asia': 11,\n",
    "    'Northern Europe': 12,\n",
    "    'Melanesia': 13,\n",
    "    'Central Asia': 14,\n",
    "    'Micronesia': 15,\n",
    "    'Polynesia': 16\n",
    "}\n",
    "\n",
    "#Split hierarchical features \n",
    "continent_train =  np.array(LED.loc[training_idx, 'continent'].map(continent_dict))\n",
    "continent_test  =  np.array(LED.loc[test_idx,     'continent'].map(continent_dict))\n",
    "continent_val   =  np.array(LED.loc[val_idx,      'continent'].map(continent_dict))\n",
    "\n",
    "subregion_train =  np.array(LED.loc[training_idx, 'subregion'].map(subregion_dict))\n",
    "subregion_test  =  np.array(LED.loc[test_idx,     'subregion'].map(subregion_dict))\n",
    "subregion_val   =  np.array(LED.loc[val_idx,      'subregion'].map(subregion_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a57979",
   "metadata": {},
   "source": [
    "## Two-level hierarichal model (Report model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017f3da0",
   "metadata": {},
   "source": [
    "In this section we define the two-level hierarichical model which is presented in the report. Both alpha and beta are on the second level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de69590",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hierarchical_model_TwoLevel_AB(X,continents,developed,num_hir, obs=None):\n",
    "    n_ind = num_hir\n",
    "    n_dev = 2\n",
    "    \n",
    "    alpha_mu = pyro.sample(\"alpha_mu\", dist.Normal(0., 1.))        # Hyper-Prior for the bias mean\n",
    "    alpha_sigma  = pyro.sample(\"alpha_sigma\", dist.HalfCauchy(2.)) # Hyper-Prior for the bias standard deviation\n",
    "    \n",
    "    beta_mu = pyro.sample(\"beta_mu\", dist.Normal(0., 1.))          # Hyper-Prior for the bias mean\n",
    "    beta_sigma  = pyro.sample(\"beta_sigma\", dist.HalfCauchy(2.))   # Hyper-Prior for the bias standard deviation\n",
    "\n",
    "    sigma = pyro.sample(\"sigma\", dist.HalfCauchy(2.))              # Prior for the observation variance\n",
    "\n",
    "    with pyro.plate(\"developed\", n_ind):\n",
    "        \n",
    "        with pyro.plate(\"continents\", n_dev):                      # Drawing parameters for each of the groupings\n",
    "            alpha = pyro.sample(\"alpha\", dist.Normal(alpha_mu, alpha_sigma).to_event()) \n",
    "            beta = pyro.sample(\"beta\", dist.Normal(beta_mu*torch.ones(X.shape[1]), \n",
    "                                               beta_sigma*torch.ones(X.shape[1])).to_event(1)) \n",
    "\n",
    "    with pyro.plate(\"data\"):\n",
    "        mu = alpha[developed,continents] + torch.sum(torch.mul(X,beta[developed,continents]),dim=1)\n",
    "        y = pyro.sample(\"y\", dist.Normal(mu,sigma).to_event(), obs=obs)\n",
    "        \n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f203baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert to torch tensors for MCMC\n",
    "developed_train_pyro = torch.tensor(developed_train.astype(\"long\")).long()  \n",
    "developed_test_pyro = torch.tensor(developed_test.astype(\"long\")).long()\n",
    "developed_val_pyro = torch.tensor(developed_val.astype(\"long\")).long()\n",
    "\n",
    "X_train_pyro = torch.tensor(X_train).float()\n",
    "y_train_pyro = torch.tensor(y_train).float()\n",
    "\n",
    "subregion_train_pyro = torch.tensor(subregion_train).long()\n",
    "\n",
    "# Run inference in Pyro\n",
    "nuts_kernel = NUTS(hierarchical_model_TwoLevel_AB)\n",
    "mcmcTwoLevel_AB = MCMC(nuts_kernel, num_samples=1000, warmup_steps=200, num_chains=1)\n",
    "mcmcTwoLevel_AB.run(X_train_pyro,subregion_train_pyro,developed_train_pyro,17, y_train_pyro)\n",
    "\n",
    "# Show summary of inference results\n",
    "mcmcTwoLevel_AB.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597bad06",
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior_samples_twoLevel_AB = mcmcTwoLevel_AB.get_samples()\n",
    "\n",
    "beta_hat_TwoLevel_AB =torch.mean(posterior_samples_twoLevel_AB[\"beta\"], axis=0)    #Use mean approximation to get parameters \n",
    "alpha_hat_TwoLevel_AB =torch.mean(posterior_samples_twoLevel_AB[\"alpha\"], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144be416",
   "metadata": {},
   "source": [
    "#### Plotting traceplots for beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da784957",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (10, 10)\n",
    "\n",
    "for i in range(17): #Looking at traceplot for beta parameters [Change the second index to switch between developed/developing]\n",
    "    plt.subplot(9,2,i+1)\n",
    "    plt.plot(posterior_samples_twoLevel_AB[\"beta\"][::2,1,i,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277c1983",
   "metadata": {},
   "source": [
    "#### Plotting traceplots for alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d8c0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (10, 10)\n",
    "\n",
    "for i in range(17): #Looking at traceplot for alpha parameters [Change the second index to switch between developed/developing]\n",
    "    plt.subplot(9,2,i+1)\n",
    "    plt.plot(posterior_samples_twoLevel_AB[\"alpha\"][::2,1,i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5003ef0f",
   "metadata": {},
   "source": [
    "#### Looking into the autocorrelation to validate thinning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8219c794",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Looking a autocorrelation without thinning\n",
    "fig, axs = plt.subplots(9,2, figsize=(8, 20))\n",
    "\n",
    "# Iterate over data and create ACF plots in subplots\n",
    "for i in range(17):    \n",
    "    row = i // 2; col = i % 2\n",
    "    \n",
    "    ax = axs[row, col]\n",
    "    sm.graphics.tsa.plot_acf(posterior_samples_twoLevel_AB[\"beta\"][:,1,i,1], ax=ax, lags=10) #Plot ACF\n",
    "    ax.set_xlabel(\"Lag\")\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5acf5c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Looking a autocorrelation with thinning\n",
    "fig, axs = plt.subplots(9,2, figsize=(8, 20))\n",
    "\n",
    "# Iterate over data and create ACF plots in subplots\n",
    "for i in range(17):\n",
    "    row = i // 2; col = i % 2\n",
    "    \n",
    "    ax = axs[row, col]\n",
    "    sm.graphics.tsa.plot_acf(posterior_samples_twoLevel_AB[\"beta\"][::2,1,i,1], ax=ax, lags=10) #Plot ACF\n",
    "    ax.set_xlabel(\"Lag\")\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6605856d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rand1 = random.randint(0, 1)  #Get random parameter to shown in the report. \n",
    "rand2 = random.randint(0, 16)\n",
    "rand3 = random.randint(0, 14)\n",
    "\n",
    "fig, axs = plt.subplots(1,2, figsize=(5, 2))  #Define figure \n",
    "\n",
    "ax = axs[0] #Without thinning\n",
    "sm.graphics.tsa.plot_acf(posterior_samples_twoLevel_AB[\"beta\"][:,rand1,rand2,rand3], ax=ax, lags=10) #Plot ACF\n",
    "ax.set_xlabel(\"Lag\")\n",
    "ax.set_title(\"Before thinning\")\n",
    "\n",
    "ax = axs[1] #With thinning\n",
    "sm.graphics.tsa.plot_acf(posterior_samples_twoLevel_AB[\"beta\"][::2,rand1,rand2,rand3], ax=ax, lags=10) #Plot ACF\n",
    "ax.set_xlabel(\"Lag\")\n",
    "ax.set_title(\"After thinning\")\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.show()\n",
    "\n",
    "fig.savefig('ACF_plot.eps',format='eps',transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a26d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating RMSE \n",
    "beta_hat_TwoLevel_AB =torch.mean(posterior_samples_twoLevel_AB[\"beta\"][::2], axis=0)\n",
    "alpha_hat_TwoLevel_AB =torch.mean(posterior_samples_twoLevel_AB[\"alpha\"][::2], axis=0)\n",
    "\n",
    "def ErrorTestFun_Model4(X_values,cont_idx,dev_idx,y_values):   #Define function to evaluate estimed parameters of the model\n",
    "    \n",
    "    multi_row = np.multiply(X_values,beta_hat_TwoLevel_AB[dev_idx,cont_idx,:])  #Perform linear algebra \n",
    "    beta_part = torch.sum(multi_row,dim=1)\n",
    "\n",
    "    y_hat = alpha_hat_TwoLevel_AB[dev_idx,cont_idx] + beta_part\n",
    "    \n",
    "    preds = y_hat.numpy() * y_std + y_mean        #Convert back to original domain\n",
    "    y_true = y_values * y_std + y_mean\n",
    "\n",
    "    rmse = np.sqrt(np.mean((preds - y_true)**2))  #Calculate RMSE\n",
    "    \n",
    "    return rmse\n",
    "    \n",
    "rmse_train = ErrorTestFun_Model4(X_train, subregion_train,developed_train_pyro, y_train)\n",
    "rmse_test  = ErrorTestFun_Model4(X_test, subregion_test, developed_test_pyro, y_test)\n",
    "rmse_val   = ErrorTestFun_Model4(X_val, subregion_val, developed_val_pyro,  y_val)\n",
    "\n",
    "print(\"RMSE train: %.3f \\nRMSE test: %.3f \\nRMSE val: %.3f\" % (rmse_train,rmse_test,rmse_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab092c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting Beta parameters for the Developing grouping \n",
    "list_labels = LED.columns.tolist()\n",
    "num_betavalues = 15\n",
    "\n",
    "heatMap_Beta_0 = beta_hat_TwoLevel_AB[0, :, :].numpy()\n",
    "\n",
    "plt.imshow(heatMap_Beta_0,cmap='viridis') #Plot the values in a heatmap\n",
    "plt.grid(visible=False)\n",
    "plt.xticks(range(num_betavalues), list_labels[6:], rotation=90)\n",
    "plt.yticks(list(subregion_dict.values()),list(subregion_dict.keys()))\n",
    "plt.title(\"Developing\")\n",
    "\n",
    "for i in range(heatMap_Beta_0.shape[0]): #Add the number in the heatmap\n",
    "    for j in range(heatMap_Beta_0.shape[1]):\n",
    "        plt.text(j, i, f'{heatMap_Beta_0[i, j]:.2f}', ha='center', va='center', color='black')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"BetaMatrixDev0.eps\",format=\"eps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b096237",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting Beta parameters for the Developed grouping \n",
    "heatMap_Beta_1 = beta_hat_TwoLevel_AB[1, :, :].numpy()\n",
    "\n",
    "plt.imshow(heatMap_Beta_1,cmap='viridis') #Plot the values in a heatmap\n",
    "plt.grid(visible=False)\n",
    "plt.xticks(range(num_betavalues), list_labels[6:], rotation=90)\n",
    "plt.yticks(list(subregion_dict.values()),list(subregion_dict.keys()))\n",
    "plt.title(\"Developed\")\n",
    "\n",
    "for i in range(heatMap_Beta_1.shape[0]): #Add the number in the heatmap\n",
    "    for j in range(heatMap_Beta_1.shape[1]):\n",
    "        plt.text(j, i, f'{heatMap_Beta_1[i, j]:.2f}', ha='center', va='center', color='black')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"BetaMatrixDev1.eps\",format=\"eps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a625a40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating to 90%CI for each of the alpha parameters\n",
    "ci_lower_dev1 = np.array([])\n",
    "ci_upper_dev1 = np.array([])\n",
    "\n",
    "for i in range(17): #For developed sub-regions\n",
    "    ci_lower_dev1 = np.append(ci_lower_dev1, np.percentile(posterior_samples_twoLevel_AB[\"alpha\"][::2,1,i], (100 - 90) / 2))\n",
    "    ci_upper_dev1 = np.append(ci_upper_dev1, np.percentile(posterior_samples_twoLevel_AB[\"alpha\"][::2,1,i], 100 - (100 - 90) / 2))\n",
    "    \n",
    "ci_lower_dev0 = np.array([])\n",
    "ci_upper_dev0 = np.array([])\n",
    "\n",
    "for i in range(17): #For developing sub-regions\n",
    "    ci_lower_dev0 = np.append(ci_lower_dev0, np.percentile(posterior_samples_twoLevel_AB[\"alpha\"][::2,0,i], (100 - 90) / 2))\n",
    "    ci_upper_dev0 = np.append(ci_upper_dev0, np.percentile(posterior_samples_twoLevel_AB[\"alpha\"][::2,0,i], 100 - (100 - 90) / 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a27f799",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the alpha parameter estimates with a 90% CI \n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "plt.subplot(2,1,1) #Alpha parameters for developed sub-regions \n",
    "plt.fill_between(list(subregion_dict.values()),ci_lower_dev1,ci_upper_dev1,alpha=0.3,label=\"Developed\",color=\"lightblue\")\n",
    "plt.plot(alpha_hat_TwoLevel_AB[1, :],linestyle=\"--\",color=\"black\",linewidth=1.0)\n",
    "plt.xticks(np.arange(17))\n",
    "plt.tick_params(axis='x', labelbottom=False)\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2,1,2) #Alpha parameters for developing sub-regions \n",
    "plt.fill_between(list(subregion_dict.values()),ci_lower_dev0,ci_upper_dev0,alpha=0.3,label=\"Developing\",color=\"lightcoral\")\n",
    "plt.plot(alpha_hat_TwoLevel_AB[0, :],linestyle=\"--\",color=\"black\",linewidth=1.0)\n",
    "plt.xticks(list(subregion_dict.values()), list(subregion_dict.keys()))\n",
    "plt.xticks(rotation=90)\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig('alpha_par_res.eps',format='eps')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
